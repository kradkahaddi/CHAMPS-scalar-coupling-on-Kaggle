{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor as lgbm\n\nEncode = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n#from mpl_toolkits.mplot3d import Axes3D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.DataFrame({'A':[1,2,3],'B':[4,5,6],'C':[7,8,9]})\nt.apply(np.sum,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"\nUser defined functions\n\"\"\"\n#This function returns z-scaled values along with mean and std\ndef normalise(df):\n    return (df - np.mean(df))/np.mean(df), np.mean(df), np.std(df)\n\n#This function returns electronegativity values\ndef nega(df):\n    nega_dict = {'C':2.55, 'H':2.2, 'N':3.04, 'O':3.44, 'F':3.98}\n    return [nega_dict[var] for var in df]\n\n#This function returns covalent radius for each atom\ndef radii(df):\n    radii_dict = {'C':73, 'H':31, 'N':71, 'O':66, 'F':57}\n    return [radii_dict[var] for var in df]\n\n#This function returns the bondlength distance\ndef distance (df):\n    return np.sqrt((df.x1 - df.x0)**2 + (df.y1 - df.y0)**2 + (df.z1 - df.z0)**2)\n\n# def EN_vectors(df):\ndef norm_ENV (length, coord, EN):\n    return  EN*coord/length\n\ndef checkdir (x0,y0,z0,x1,y1,z1):\n    diff = np.sqrt((x1 - x0)**2 + (y1 - y0)**2 + (z1 - z0)**2)\n    summ = np.sqrt((x1 + x0)**2 + (y1 + y0)**2 + (z1 + z0)**2)\n    if summ >= diff:\n        return 1\n    else:\n        return -1\n    #if len(sum) > len\n    \n# def ENV_res(row):\n#     temp = train_X[['V_x','V_y','V_z']].loc[(molecule_name == row.molecule_name) & (atom_index0 == row.atom_index0)].apply(np.sum,axis =0)\n#     return temp\n\ndef ENV_res_x(row):\n    temp = train_X.loc[(train_X.molecule_name == row[0]) & (train_X.atom_index0 == row[2])]\n    return np.sum(temp.V_x)\n\ndef ENV_res_y(row):\n    temp = train_X.loc[(train_X.molecule_name == row.molecule_name) & (train_X.atom_index0 == row.atom_index0)]\n    return np.sum(temp.V_y)\n\ndef ENV_res_z(row):\n    temp = train_X.loc[(train_X.molecule_name == row.molecule_name) & (train_X.atom_index0 == row.atom_index0)]\n    return np.sum(temp.V_z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial_test = pd.DataFrame()\ntrial_test.assign([x,y,z]= [1,2,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" LOG MAE score calculator\n    taken from the public kernel - https://www.kaggle.com/marcelotamashiro/lgb-public-kernels-plus-more-features @marcelotomashiro\n\"\"\"\n\ndef comp_score (y_true, y_pred, jtype):\n    df = pd.DataFrame()\n    df['y_true'] , df['y_pred'], df['jtype'] = y_true , y_pred, jtype\n    score = 0 \n    for t in jtype.unique():\n        score_jtype = np.log(mean_absolute_error(df[df.jtype==t]['y_true'],df[df.jtype==t]['y_pred']))\n        score += score_jtype\n        print(f'{t} : {score_jtype}')\n    score /= len(jtype.unique())\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/champs-scalar-coupling/train.csv\")\nstructure = pd.read_csv(\"../input/champs-scalar-coupling/structures.csv\")\n\ntrain_X = train[train.columns[1:-1]]\ntrain_y = train[train.columns[-1]]\n\ntrain_X = pd.merge(train_X,structure, left_on=['molecule_name','atom_index_0'], right_on=['molecule_name','atom_index'], how='left',suffixes =('0','1'))\ntrain_X = train_X.assign(EN0 = nega(train_X.atom))\ntrain_X = pd.merge(train_X,structure, left_on=['molecule_name','atom_index_1'], right_on=['molecule_name','atom_index'], how='left',suffixes =('0','1'))\ntrain_X = train_X.assign(EN1 = nega(train_X.atom1))\ntrain_X = train_X.assign(REN = train_X.EN1 - train_X.EN0)\ntrain_X = train_X.drop(['atom_index_1','atom_index_0'], axis =1)\n\n# distance_norm, distance_mean, distance_std = normalise(distance(train_X))\n\n\n# train_X = train_X.assign(x1x0 = train_X.x1 - train_X.x0)\n# train_X = train_X.assign(y1y0 = train_X.y1 - train_X.y0)\n# train_X = train_X.assign(z1z0 = train_X.z1 - train_X.z0)\n\ndistance_V = distance(train_X)\ntrain_X = train_X.assign(V_x = norm_ENV(distance_V, train_X.x1 - train_X.x0, (train_X.EN1 - train_X.EN0)/distance_V))\ntrain_X = train_X.assign(V_y = norm_ENV(distance_V, train_X.y1 - train_X.y0, (train_X.EN1 - train_X.EN0)/distance_V))\ntrain_X = train_X.assign(V_z = norm_ENV(distance_V, train_X.z1 - train_X.z0, (train_X.EN1 - train_X.EN0)/distance_V))\n\n#TEST DATA\ntest = pd.read_csv(\"../input/champs-scalar-coupling/test.csv\")\ntest_X = test\n\ntest_X = pd.merge(test_X, structure, left_on=['molecule_name','atom_index_0'], right_on=['molecule_name','atom_index'], how='left',suffixes =('0','1'))\ntest_X = test_X.assign(EN0 = nega(test_X.atom))\ntest_X = pd.merge(test_X, structure, left_on=['molecule_name','atom_index_1'], right_on=['molecule_name','atom_index'], how='left',suffixes =('0','1'))\ntest_X = test_X.assign(EN1 = nega(test_X.atom1))\ntest_X = test_X.assign(REN = train_X.EN1 - train_X.EN0)\ntest_X = test_X.drop(['atom_index_1','atom_index_0'], axis =1)\n\ndistance_y = distance(test_X)\ntest_X = test_X.assign(V_x = norm_ENV(distance_y, test_X.x1 - test_X.x0, (test_X.EN1 - test_X.EN0)/distance_y))\ntest_X = test_X.assign(V_y = norm_ENV(distance_y, test_X.z1 - test_X.y0, (test_X.EN1 - test_X.EN0)/distance_y))\ntest_X = test_X.assign(V_z = norm_ENV(distance_y, test_X.z1 - test_X.z0, (test_X.EN1 - test_X.EN0)/distance_y))\n\n\ntrain_X.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_X = train_X.assign(ENV_x = train_X.values.apply(ENV_res_x, axis =1))\n# train_X = train_X.assign(ENV_y = train_X.values.apply(ENV_res_y, axis =1))\n# train_X = train_X.assign(ENV_z = train_X.values.apply(ENV_res_z, axis =1))\n\ntrain_X = train_X.assign(ENV_x =[ENV_res_x(x) for x in train_X.values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_X.loc[(train_X.molecule_name == train_X.molecule_name.unique()[0])]\n# train_X.loc[((train_X.atom_index0 == 4))]\n# train_X.loc[(train_X.molecule_name == train_X.molecule_name.unique()[0]) & (train_X.atom_index0 == 3)]\n# train_X.loc[(train_X.molecule_name == train_X.molecule_name.unique()[0]) & ((train_X.atom_index0 == 3) | (train_X.atom_index1 == 3))]\ntrain_X.loc[(train_X.molecule_name == train_X.molecule_name.unique()[7]) & ((train_X.atom0 == 'C') | (train_X.atom1 == 'C'))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(train.type,train.scalar_coupling_constant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training set\nX = pd.DataFrame()\n#assigning z scale of bondlength\nCL, CL_mean, CL_std = normalise(distance(train_X))\nX = X.assign(coupling_length = CL)\n\n\nEN1, EN_mean1, EN_std1 = normalise(nega(train_X.atom1))\nX = X.assign(E_neg1 = EN1)\n\nradi1, radi_mean1, radi_std1 = normalise(radii(train_X.atom1))\nX = X.assign(a_radius1 = radi1)\n\nX = X.assign(norm_EN1 = train_X.REN/distance_V) #same as magnitude of the ENV vector\n\nX = X.assign(V_x = train_X.V_x)\nX = X.assign(V_y = train_X.V_y)\nX = X.assign(V_z = train_X.V_z)\n\n# X = X.assign(ENV1_mag = np.sqrt(train_X.V_x**2 + train_X.V_y**2 + train_X.V_z**2)) #same as norm_EN1\n\ntype_col  = pd.DataFrame(Encode.fit_transform(np.array(train_X['type']).reshape(-1,1)))\n\nX = pd.concat([X, type_col], axis=1)\n\nX.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1JHC = X.loc[train_X.type == '1JHC'] \nX_2JHH = X.loc[train_X.type == '2JHH']\nX_1JHN = X.loc[train_X.type == '1JHN']\nX_rest = X.loc[(train_X.type != '1JHC') & (train_X.type != '2JHH') & (train_X.type != '1JHN')]\n\ny_1JHC = train_y.loc[train_X.type == '1JHC']\ny_2JHH = train_y.loc[train_X.type == '2JHH']\ny_1JHN = train_y.loc[train_X.type == '1JHN']\ny_rest = train_y.loc[(train_X.type != '1JHC') & (train_X.type != '2JHH') & (train_X.type != '1JHN')]\n\nnorm_y_1JHC, y_1JHC_mean, y_1JHC_std = normalise(y_1JHC)\nnorm_y_2JHH, y_2JHH_mean, y_2JHH_std = normalise(y_2JHH)\nnorm_y_1JHN, y_1JHN_mean, y_1JHN_Std = normalise(y_1JHN)\nnorm_y_rest, y_rest_mean, y_rest_std = normalise(y_rest)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rest_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t_1JHC, X_v_1JHC, y_t_1JHC, y_v_1JHC = train_test_split(X_1JHC,norm_y_1JHC, test_size=0.3, random_state= 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom lightgbm import LGBMRegressor\nmy_lgbm_model = LGBMRegressor(max_depth = -1, n_estimators =400, learning_rate =0.01, random_state =0)\nmy_lgbm_model.fit(X_train, y_train, eval_metric ='mae')\nval_predictions_lgbm = my_lgbm_model.predict(X_valid)\n\n\"\"\"\nfrom lightgbm import LGBMRegressor as lgbm\n\nlgbm_1JHC = lgbm(max_depth = -1, n_estimators =5000, learning_rate =0.1, random_state =0)\nlgbm_1JHC.fit(X_t_1JHC, y_t_1JHC, eval_metric ='mae')\n\npredictions_1JHC = lgbm_1JHC.predict(X_v_1JHC)\n\nmean_absolute_error(predictions_1JHC, y_v_1JHC)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t_rest, X_v_rest, y_t_rest, y_v_rest = train_test_split(X_rest,norm_y_rest, test_size=0.3, random_state= 0)\nlgbm_rest = lgbm(max_depth = -1, learning_rate = 0.1, random_state =0)\nlgbm_rest.fit(X_t_rest, y_t_rest, eval_metric ='mae')\n\npredictions_rest = lgbm_rest.predict(X_v_rest)\n\nmean_absolute_error(predictions_rest, y_v_rest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}